# -*- coding: utf-8 -*-
"""chatbot_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nrbdYs67VUF2RS9uyK_c8LpQuZdXdcW6
"""

import streamlit as st
import os
from dotenv import load_dotenv

# Importaciones modulares corregidas
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
# Si necesitas cargar el PDF: from langchain_community.document_loaders import PyPDFLoader
# Si necesitas dividir texto: from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- CONFIGURACIN Y UTILIDADES ---

# Carga las variables de entorno (como GOOGLE_API_KEY)
load_dotenv()

# El c贸digo debe buscar la clave en st.secrets
if 'GOOGLE_API_KEY' not in st.secrets:
    st.error("Error: La clave GOOGLE_API_KEY no se encontr贸 en Streamlit Secrets.")
    st.stop()

# Asigna la clave como variable de entorno para que el SDK de LangChain/Google la encuentre
os.environ['GOOGLE_API_KEY'] = st.secrets['GOOGLE_API_KEY']


@st.cache_resource
def load_vector_store():
    """
    Carga el 铆ndice FAISS pre-construido.
    Nota: Debe haber ejecutado el c贸digo anterior para crear 'faiss_index_local'
    """
    try:
        # Los embeddings son necesarios para cargar el 铆ndice FAISS
        embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        vector_store = FAISS.load_local("faiss_index_local", embeddings, allow_dangerous_deserialization=True)
        return vector_store
    except Exception as e:
        st.error(f"Error al cargar la base de datos vectorial FAISS. Aseg煤rate de que el directorio 'faiss_index_local' exista. {e}")
        st.stop()

# --- QUITAR ESTE CACHE (隆Corrige el Error!) ---
# Ya no tiene el decorador @st.cache_resource
def setup_qa_chain(vector_db): 
    """
    Configura el LLM (Gemini) y la cadena de recuperaci贸n conversacional (RAG).
    """
    # Inicializa el LLM de Google
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)
    
    # Crea el recuperador (Retriever)
    retriever = vector_db.as_retriever()

    # Crea la cadena RAG con historial
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=False  
    )
    return qa_chain

# --- LGICA PRINCIPAL DE STREAMLIT ---

def main():
    """
    Funci贸n principal de la aplicaci贸n Streamlit.
    """
    st.set_page_config(page_title=" Chatbot RAG con Gemini", page_icon="")

    # T铆tulo y Descripci贸n con formato "bonito"
    st.title(" Chatbot RAG | Reportes Documentales")
    st.markdown("""
        **Pregunta a tus documentos** usando el modelo **Gemini 2.5 Flash**.
        La IA recupera informaci贸n directamente de la base de datos vectorial (FAISS)
        que has creado.
    """)
    st.divider()

    # Inicializaci贸n del historial de chat en el estado de la sesi贸n
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # 1. Cargar Base de Datos y Configurar Cadena
    vector_db = load_vector_store()
    qa_chain = setup_qa_chain(vector_db)

    # 2. Mostrar mensajes de chat anteriores
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 3. Manejar la entrada del usuario
    if prompt := st.chat_input("Escribe tu pregunta aqu铆..."):
        # Agregar el mensaje del usuario al historial
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Buscando y generando respuesta con Gemini..."):

                # Prepara el historial para la cadena (formato de tuplas)
                chat_history = [(m["content"], m["content"]) for m in st.session_state.messages if m["role"] != "user"]

                # Invoca la cadena RAG
                result = qa_chain.invoke({"question": prompt, "chat_history": chat_history})
                response = result['answer']

                # Muestra la respuesta del asistente
                st.markdown(response)

        # Agregar la respuesta del asistente al historial
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()
