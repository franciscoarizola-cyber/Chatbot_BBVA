# -*- coding: utf-8 -*-
"""chatbot_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-qCRvqILogRQ6iXvKI5hnj206c0durYO
"""

import streamlit as st
import os
from dotenv import load_dotenv

# Importaciones modulares corregidas
# LangChain Classic para la cadena conversacional
from langchain_classic.chains import ConversationalRetrievalChain
# Google AI SDK para modelos y embeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
# LangChain Community para la base de datos vectorial FAISS
from langchain_community.vectorstores import FAISS

# --- CONFIGURACIN Y UTILIDADES ---

# Carga las variables de entorno (solo 煤tiles para pruebas locales, Streamlit Cloud usa secrets)
load_dotenv()

# Manejo de la Clave API
# ---------------------------------------------------------------------------------------
# NOTA: st.secrets es la forma segura de acceder a la clave en Streamlit Cloud.
# Esto asegura que la clave se inyecte en el entorno de OS para que el SDK de LangChain la encuentre.
if 'GOOGLE_API_KEY' not in st.secrets:
    st.error("Error: La clave GOOGLE_API_KEY no se encontr贸 en Streamlit Secrets. Aseg煤rate de configurarla en el panel de despliegue.")
    st.stop()

# Asigna la clave de Streamlit Secrets como variable de entorno
os.environ['GOOGLE_API_KEY'] = st.secrets['GOOGLE_API_KEY']
# ---------------------------------------------------------------------------------------

# --- Utilidad para la Actualizaci贸n Autom谩tica del Cach茅 ---

def get_faiss_index_hash(db_path="faiss_index_local"):
    """
    Retorna el timestamp de modificaci贸n del archivo principal de FAISS.
    Este valor cambiar谩 cada vez que subas nuevos datos a GitHub, forzando
    la invalidaci贸n de la cach茅 de Streamlit.
    """
    # Usamos index.faiss como indicador de que toda la carpeta ha cambiado
    index_file_path = os.path.join(db_path, "index.faiss")
    if os.path.exists(index_file_path):
        # Devuelve el tiempo de la 煤ltima modificaci贸n
        return os.path.getmtime(index_file_path)
    # Si el 铆ndice no existe (ej. primer despliegue sin datos), devuelve 0
    return 0

# --- Carga de Base de Datos (Cacheada con L贸gica de Actualizaci贸n) ---

@st.cache_resource
def load_vector_store(index_hash):
    """
    Carga el 铆ndice FAISS pre-construido.
    El par谩metro 'index_hash' garantiza la invalidaci贸n autom谩tica del cach茅
    cuando los archivos del 铆ndice FAISS son actualizados en GitHub.
    """
    try:
        # Los embeddings son necesarios para cargar el 铆ndice FAISS
        embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
        vector_store = FAISS.load_local("faiss_index_local", embeddings, allow_dangerous_deserialization=True)
        return vector_store
    except Exception as e:
        st.error(f"Error al cargar la base de datos FAISS. Aseg煤rate de que la carpeta 'faiss_index_local' est茅 en el repositorio. Detalle: {e}")
        st.stop()

# --- Configuraci贸n de la Cadena RAG (Sin Cach茅) ---

# NO cacheamos esta funci贸n porque recibe un objeto FAISS no hasheable
def setup_qa_chain(vector_db):
    """
    Configura el LLM (Gemini) y la cadena de recuperaci贸n conversacional (RAG).
    """
    # Inicializa el LLM de Google
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)

    # Crea el recuperador (Retriever)
    retriever = vector_db.as_retriever()

    # Crea la cadena RAG con historial
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=False
    )
    return qa_chain

# --- LGICA PRINCIPAL DE STREAMLIT ---

def main():
    """
    Funci贸n principal de la aplicaci贸n Streamlit.
    """
    st.set_page_config(page_title=" Chatbot BBVA Research", page_icon="")

    # T铆tulo y Descripci贸n con formato "bonito"
    st.title(" Chatbot BBVA Research | Estudios Econ贸micos")
    st.markdown("""
        **Pregunta a tus documentos** usando el modelo **Gemini 2.5 Flash**.
        Consulte por informaci贸n relacionada a nuestros reportes de PBI, Inflaci贸n, Mercado Laboral, Expectativas
        Macroecon贸micas, Situaci贸n Fiscal, entre otros.
    """)
    st.divider()

    # Inicializaci贸n del historial de chat en el estado de la sesi贸n
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # 1. Cargar Base de Datos y Configurar Cadena
    # Obtenemos el hash actual para la invalidaci贸n autom谩tica
    current_index_hash = get_faiss_index_hash()
    vector_db = load_vector_store(current_index_hash)
    qa_chain = setup_qa_chain(vector_db)

    # 2. Mostrar mensajes de chat anteriores
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 3. Manejar la entrada del usuario
    if prompt := st.chat_input("Escribe tu pregunta aqu铆..."):
        # Agregar el mensaje del usuario al historial
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Buscando y generando respuesta con Gemini..."):

                # Prepara el historial para la cadena (formato de tuplas: (pregunta, respuesta))
                # Nota: Recorremos el historial para construir el formato que LangChain espera.
                history_list = []
                temp_messages = [m for m in st.session_state.messages if m["role"] != "system"]

                # Construir el historial (pregunta, respuesta)
                for i in range(0, len(temp_messages) - 1, 2):
                    if temp_messages[i]["role"] == "user" and temp_messages[i+1]["role"] == "assistant":
                         history_list.append((temp_messages[i]["content"], temp_messages[i+1]["content"]))

                # Invoca la cadena RAG
                result = qa_chain.invoke({"question": prompt, "chat_history": history_list})
                response = result['answer']

                # Muestra la respuesta del asistente
                st.markdown(response)

        # Agregar la respuesta del asistente al historial
        st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    main()